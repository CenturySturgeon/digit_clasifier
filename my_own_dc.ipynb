{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb1712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "! [ -e /content ] && pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4b759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c9df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d40a1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "094b7b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('../../.fastai/data/mnist_png/testing/5'),\n",
       " Path('../../.fastai/data/mnist_png/testing/3'),\n",
       " Path('../../.fastai/data/mnist_png/testing/7'),\n",
       " Path('../../.fastai/data/mnist_png/testing/4'),\n",
       " Path('../../.fastai/data/mnist_png/testing/2'),\n",
       " Path('../../.fastai/data/mnist_png/testing/1'),\n",
       " Path('../../.fastai/data/mnist_png/testing/9'),\n",
       " Path('../../.fastai/data/mnist_png/testing/0'),\n",
       " Path('../../.fastai/data/mnist_png/testing/8'),\n",
       " Path('../../.fastai/data/mnist_png/testing/6')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_folder = Path('../../.fastai/data/mnist_png/testing')\n",
    "training_folder = Path('../../.fastai/data/mnist_png/training')\n",
    "\n",
    "[x for x in testing_folder.iterdir() if x.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "331fd357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Path('../../.fastai/data/mnist_png/training/0'),\n",
       " Path('../../.fastai/data/mnist_png/testing/3'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_testing_folder = testing_folder.ls().sorted()\n",
    "sorted_training_folder = training_folder.ls().sorted()\n",
    "\n",
    "sorted_training_folder[0], sorted_testing_folder[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3537f2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6131, 28, 28]),\n",
       " torch.Size([6265, 28, 28]),\n",
       " torch.Size([12396, 784]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threes_tensors = [tensor(Image.open(o)) for o in (training_folder/'3').ls().sorted()]\n",
    "stacked_threes = torch.stack(threes_tensors).float()/255\n",
    "\n",
    "sevens_tensors = [tensor(Image.open(o)) for o in (training_folder/'7').ls().sorted()]\n",
    "stacked_sevens = torch.stack(sevens_tensors).float()/255\n",
    "\n",
    "#reshape the tensors and concatenate them in a new one\n",
    "test_tensor = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n",
    "\n",
    "stacked_threes.shape, stacked_sevens.shape, test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a4ff196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_tensors(path_to_images):\n",
    "    \"\"\"A function that returns a stacked Gray Scaled tensor\"\"\"\n",
    "    tensors = [tensor(Image.open(o)) for o in (path_to_images).ls().sorted()]\n",
    "    # Transforms to grayscale (/255) and Normalizes to mean 0 ((x-0.5)/0.5)\n",
    "    stacked_tensors = ((torch.stack(tensors).float()/255)-.5)/.5\n",
    "    #Original line, only converts to grayscale \n",
    "    #stacked_tensors = torch.stack(tensors).float()/255\n",
    "\n",
    "    return stacked_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9347be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the function works as expected\n",
    "training_stacked_threes = stacked_tensors(sorted_training_folder[3])\n",
    "training_stacked_sevens = stacked_tensors(sorted_training_folder[7])\n",
    "\n",
    "training_stacked_threes.shape, training_stacked_sevens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3866075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensors are equal.\n"
     ]
    }
   ],
   "source": [
    "#Corroborate equality of stacked tensors\n",
    "if torch.equal(stacked_threes, training_stacked_threes) and torch.equal(stacked_sevens, training_stacked_sevens) :\n",
    "    print(\"The tensors are equal.\")\n",
    "else:\n",
    "    print(\"The tensors are not equal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00a60e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash_tensors(list_of_paths):\n",
    "    listed_tensors = []\n",
    "    for i in list_of_paths:\n",
    "        listed_tensors.append(stacked_tensors(i))\n",
    "    squashed_tensors = torch.cat(listed_tensors).view(-1, 28*28)\n",
    "    return squashed_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36fff3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash_tensors(list_of_paths):\n",
    "    squashed_tensors = torch.cat([stacked_tensors(o) for o in list_of_paths]).view(-1, 28*28)\n",
    "    return squashed_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a02dae31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the squashed tensors are equal\n",
    "squashed_threes_and_sevens = squash_tensors([sorted_training_folder[3], sorted_training_folder[7]])\n",
    "\n",
    "squashed_threes_and_sevens.shape == test_tensor.shape, torch.equal(squashed_threes_and_sevens, test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a08522a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 784])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squashed_training_tensors = squash_tensors(sorted_training_folder)\n",
    "\n",
    "squashed_training_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d2ad690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1010, 28, 28]),\n",
       " torch.Size([1028, 28, 28]),\n",
       " torch.Size([2038, 784]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid3s_tensor = torch.stack([tensor(Image.open(o)) for o in (sorted_testing_folder[3]).ls().sorted()]).float()/255\n",
    "valid7s_tensor = torch.stack([tensor(Image.open(o)) for o in (sorted_testing_folder[7]).ls().sorted()]).float()/255\n",
    "valid37s_tensor = torch.cat([valid3s_tensor, valid7s_tensor]).view(-1, 28*28)\n",
    "\n",
    "valid3s_tensor.shape, valid7s_tensor.shape, valid37s_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e639bd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1010, 28, 28]),\n",
       " torch.Size([1028, 28, 28]),\n",
       " torch.Size([2038, 784]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_stacked_threes = stacked_tensors(sorted_testing_folder[3])\n",
    "validation_stacked_sevens = stacked_tensors(sorted_testing_folder[7])\n",
    "squashed_valid3and7s_tensor = squash_tensors([sorted_testing_folder[3], sorted_testing_folder[7]])\n",
    "\n",
    "validation_stacked_threes.shape, validation_stacked_sevens.shape, squashed_valid3and7s_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b060be6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensors are equal.\n"
     ]
    }
   ],
   "source": [
    "#Corroborate equality of stacked tensors\n",
    "if torch.equal(valid3s_tensor, validation_stacked_threes) and torch.equal(valid7s_tensor, validation_stacked_sevens) :\n",
    "    print(\"The tensors are equal.\")\n",
    "else:\n",
    "    print(\"The tensors are not equal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73319112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the squashed tensors are equal\n",
    "squashed_valid3and7s_tensor.shape == valid37s_tensor.shape, torch.equal(squashed_valid3and7s_tensor, valid37s_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e660b681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 784])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squashed_validation_tensors = squash_tensors(sorted_testing_folder)\n",
    "\n",
    "squashed_validation_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d96920b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test = tensor([1]*len(threes_tensors) + [0]*len(sevens_tensors)).unsqueeze(1)\n",
    "labels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bf63ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the validation labels tensor\n",
    "def getlabels(folders):\n",
    "    list_of_labels = []\n",
    "    for o in range(len(folders)):\n",
    "        list_of_labels = list_of_labels + [o]*len(folders[o].ls())\n",
    "    labels_tensor = tensor(list_of_labels)\n",
    "    return labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d693c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List comprehension version of the code block above\n",
    "#start_time = time.time()\n",
    "#a = [o for o in range(len(sorted_testing_folder)) for _ in range(len(sorted_testing_folder[o].ls()))]\n",
    "#tensor_a = torch.tensor(a)\n",
    "#for_loop_time = time.time() - start_time\n",
    "#for_loop_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "116ea797",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_labels_tensor = getlabels(sorted_testing_folder)\n",
    "\n",
    "training_labels_tensor = getlabels(sorted_training_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4861b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = list(zip(squashed_validation_tensors, validation_labels_tensor))\n",
    "\n",
    "training_dataset = list(zip(squashed_training_tensors, training_labels_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57cb3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataloader = DataLoader(validation_dataset, batch_size = 256, shuffle=True)\n",
    "\n",
    "training_dataloader = DataLoader(training_dataset, batch_size = 256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d9109a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fbe1c640550>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.Grayscale(), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize([0.5], [0.5])]\n",
    ")\n",
    "\n",
    "testing_dataset = torchvision.datasets.ImageFolder((testing_folder).as_posix(), transform = transform)\n",
    "testing_dataloader = torch.utils.data.DataLoader(testing_dataset, batch_size=60)\n",
    "\n",
    "testing_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38cc0d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": training_dataloader,\n",
    "    \"validation\": validation_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c910108",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,10),\n",
    "    nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7af816e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nb_epoch is our number of epochs, meaning the number of complete passes through the training dataset.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "lr = 1e-2\n",
    "nb_epoch = 50\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a8ac2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(pytorch_net.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2c92698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, num_epochs=10):\n",
    "    #liveloss = PlotLosses() Live training plot generic API\n",
    "    epochno = 0\n",
    "    model = model.to(device) # Moves and/or casts the parameters and buffers to device.\n",
    "    \n",
    "    for epoch in range(num_epochs): # Number of passes through the entire training & validation datasets\n",
    "        logs = {}\n",
    "        for phase in ['train', 'validation']: # First train, then validate\n",
    "            if phase == 'train':\n",
    "                model.train() # Set the module in training mode\n",
    "            else:\n",
    "                model.eval() # Set the module in evaluation mode\n",
    "\n",
    "            running_loss = 0.0 # keep track of loss\n",
    "            running_corrects = 0 # count of carrectly classified inputs\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device) # Perform Tensor device conversion\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs) # forward pass through network\n",
    "                loss = criterion(outputs, labels) # Calculate loss\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad() # Set all previously calculated gradients to 0\n",
    "                    loss.backward() # Calculate gradients\n",
    "                    optimizer.step() # Step on the weights using those gradient w -=  gradient(w) * lr\n",
    "\n",
    "                _, preds = torch.max(outputs, 1) # Get model's predictions\n",
    "                running_loss += loss.detach() * inputs.size(0) # multiply mean loss by the number of elements\n",
    "                running_corrects += torch.sum(preds == labels.data) # add number of correct predictions to total\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset) # get the \"mean\" loss for the epoch\n",
    "            epoch_acc = running_corrects.float() / len(dataloaders[phase].dataset) # Get proportion of correct predictions\n",
    "            \n",
    "            # Logging\n",
    "            prefix = ''\n",
    "            if phase == 'validation':\n",
    "                prefix = 'val_'\n",
    "\n",
    "            logs[prefix + 'log loss'] = epoch_loss.item()\n",
    "            logs[prefix + 'accuracy'] = epoch_acc.item()\n",
    "        print('Epoch: ', epochno,' loss: ', epoch_loss.item(), ' accuracy: ', epoch_acc.item())\n",
    "        epochno += 1\n",
    "        #liveloss.update(logs) Update logs\n",
    "        #liveloss.send()  draw, display stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1f2e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  loss:  2.2914676666259766  accuracy:  0.16939999163150787\n",
      "Epoch:  1  loss:  2.2683420181274414  accuracy:  0.21400000154972076\n",
      "Epoch:  2  loss:  2.219364881515503  accuracy:  0.38589999079704285\n",
      "Epoch:  3  loss:  2.0795624256134033  accuracy:  0.36890000104904175\n",
      "Epoch:  4  loss:  1.7697268724441528  accuracy:  0.49619999527931213\n",
      "Epoch:  5  loss:  1.2530940771102905  accuracy:  0.676099956035614\n",
      "Epoch:  6  loss:  0.8783851265907288  accuracy:  0.7565999627113342\n",
      "Epoch:  7  loss:  0.7036246061325073  accuracy:  0.7924999594688416\n",
      "Epoch:  8  loss:  0.6151942014694214  accuracy:  0.8186999559402466\n",
      "Epoch:  9  loss:  0.5570011138916016  accuracy:  0.8370999693870544\n",
      "Epoch:  10  loss:  0.5102318525314331  accuracy:  0.8513000011444092\n",
      "Epoch:  11  loss:  0.46997132897377014  accuracy:  0.8622999787330627\n",
      "Epoch:  12  loss:  0.4354182481765747  accuracy:  0.8750999569892883\n",
      "Epoch:  13  loss:  0.4054580330848694  accuracy:  0.883899986743927\n",
      "Epoch:  14  loss:  0.38082531094551086  accuracy:  0.8898999691009521\n",
      "Epoch:  15  loss:  0.3643023669719696  accuracy:  0.8947999477386475\n",
      "Epoch:  16  loss:  0.3473077416419983  accuracy:  0.9009999632835388\n",
      "Epoch:  17  loss:  0.32947346568107605  accuracy:  0.9059999585151672\n",
      "Epoch:  18  loss:  0.3174160420894623  accuracy:  0.9096999764442444\n",
      "Epoch:  19  loss:  0.3077854514122009  accuracy:  0.9131999611854553\n",
      "Epoch:  20  loss:  0.29977139830589294  accuracy:  0.9164999723434448\n",
      "Epoch:  21  loss:  0.28871458768844604  accuracy:  0.9188999533653259\n",
      "Epoch:  22  loss:  0.2811724841594696  accuracy:  0.9204999804496765\n",
      "Epoch:  23  loss:  0.27726230025291443  accuracy:  0.9202999472618103\n",
      "Epoch:  24  loss:  0.26697397232055664  accuracy:  0.9247999787330627\n",
      "Epoch:  25  loss:  0.259082555770874  accuracy:  0.9271999597549438\n",
      "Epoch:  26  loss:  0.25278541445732117  accuracy:  0.9280999898910522\n",
      "Epoch:  27  loss:  0.24873758852481842  accuracy:  0.9282999634742737\n",
      "Epoch:  28  loss:  0.24252133071422577  accuracy:  0.9297999739646912\n",
      "Epoch:  29  loss:  0.23825404047966003  accuracy:  0.9314999580383301\n",
      "Epoch:  30  loss:  0.23350827395915985  accuracy:  0.9318999648094177\n",
      "Epoch:  31  loss:  0.22742247581481934  accuracy:  0.9330999851226807\n",
      "Epoch:  32  loss:  0.22499628365039825  accuracy:  0.9345999956130981\n",
      "Epoch:  33  loss:  0.21756616234779358  accuracy:  0.9369999766349792\n",
      "Epoch:  34  loss:  0.21304967999458313  accuracy:  0.9368000030517578\n",
      "Epoch:  35  loss:  0.20918405055999756  accuracy:  0.9390999674797058\n",
      "Epoch:  36  loss:  0.20622970163822174  accuracy:  0.9396999478340149\n",
      "Epoch:  37  loss:  0.20375026762485504  accuracy:  0.9404000043869019\n",
      "Epoch:  38  loss:  0.1990358680486679  accuracy:  0.9416999816894531\n",
      "Epoch:  39  loss:  0.19751368463039398  accuracy:  0.9416999816894531\n",
      "Epoch:  40  loss:  0.19315838813781738  accuracy:  0.9429000020027161\n",
      "Epoch:  41  loss:  0.1890297383069992  accuracy:  0.9448999762535095\n",
      "Epoch:  42  loss:  0.18592582643032074  accuracy:  0.9462999701499939\n",
      "Epoch:  43  loss:  0.18366587162017822  accuracy:  0.9465999603271484\n",
      "Epoch:  44  loss:  0.18173877894878387  accuracy:  0.9477999806404114\n",
      "Epoch:  45  loss:  0.17769849300384521  accuracy:  0.9477999806404114\n",
      "Epoch:  46  loss:  0.1751982569694519  accuracy:  0.949999988079071\n",
      "Epoch:  47  loss:  0.1747620701789856  accuracy:  0.948699951171875\n",
      "Epoch:  48  loss:  0.1704540103673935  accuracy:  0.9501000046730042\n",
      "Epoch:  49  loss:  0.17074383795261383  accuracy:  0.9493999481201172\n"
     ]
    }
   ],
   "source": [
    "train_model(pytorch_net, criterion, optimizer, dataloaders, nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "122744f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pytorch_net, 'models/my_own_dc_97pct.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22139857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3581019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.Grayscale(), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize([0.5], [0.5])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4050dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = torchvision.datasets.ImageFolder((training_folder).as_posix(), transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d68ce54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f9021d5ffd0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchSize = 64\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=batchSize, shuffle=True)\n",
    "\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "641affe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f9021d5ef50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing_dataset = torchvision.datasets.ImageFolder(testing_folder, transform = transform)\n",
    "#testing_dataloader = torch.utils.data.DataLoader(testing_dataset, batch_size=batchSize)\n",
    "testing_dataset = torchvision.datasets.ImageFolder((testing_folder).as_posix(), transform = transform)\n",
    "testing_dataloader = torch.utils.data.DataLoader(testing_dataset, batch_size=batchSize)\n",
    "\n",
    "testing_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db009b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"validation\": testing_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af43c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,10),\n",
    "    nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f58720c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nb_epoch is our number of epochs, meaning the number of complete passes through the training dataset.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "lr = 1e-2\n",
    "nb_epoch = 60\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07b9138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(pytorch_net.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef2c4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, num_epochs=10):\n",
    "    #liveloss = PlotLosses() Live training plot generic API\n",
    "    model = model.to(device) # Moves and/or casts the parameters and buffers to device.\n",
    "    \n",
    "    for epoch in range(num_epochs): # Number of passes through the entire training & validation datasets\n",
    "        logs = {}\n",
    "        for phase in ['train', 'validation']: # First train, then validate\n",
    "            if phase == 'train':\n",
    "                model.train() # Set the module in training mode\n",
    "            else:\n",
    "                model.eval() # Set the module in evaluation mode\n",
    "\n",
    "            running_loss = 0.0 # keep track of loss\n",
    "            running_corrects = 0 # count of carrectly classified inputs\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device) # Perform Tensor device conversion\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs) # forward pass through network\n",
    "                loss = criterion(outputs, labels) # Calculate loss\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad() # Set all previously calculated gradients to 0\n",
    "                    loss.backward() # Calculate gradients\n",
    "                    optimizer.step() # Step on the weights using those gradient w -=  gradient(w) * lr\n",
    "\n",
    "                _, preds = torch.max(outputs, 1) # Get model's predictions\n",
    "                running_loss += loss.detach() * inputs.size(0) # multiply mean loss by the number of elements\n",
    "                running_corrects += torch.sum(preds == labels.data) # add number of correct predictions to total\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset) # get the \"mean\" loss for the epoch\n",
    "            epoch_acc = running_corrects.float() / len(dataloaders[phase].dataset) # Get proportion of correct predictions\n",
    "            \n",
    "            # Logging\n",
    "            prefix = ''\n",
    "            if phase == 'validation':\n",
    "                prefix = 'val_'\n",
    "\n",
    "            logs[prefix + 'log loss'] = epoch_loss.item()\n",
    "            logs[prefix + 'accuracy'] = epoch_acc.item()\n",
    "        print('loss: ', epoch_loss.item(), ' accuracy: ', epoch_acc.item())\n",
    "        \n",
    "        #liveloss.update(logs) Update logs\n",
    "        #liveloss.send()  draw, display stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f790dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.593288242816925  accuracy:  0.8185999989509583\n",
      "loss:  0.4740932881832123  accuracy:  0.8499999642372131\n",
      "loss:  0.32599714398384094  accuracy:  0.9025999903678894\n",
      "loss:  0.30259695649147034  accuracy:  0.9103999733924866\n",
      "loss:  0.25638362765312195  accuracy:  0.923799991607666\n",
      "loss:  0.22863425314426422  accuracy:  0.9310999512672424\n",
      "loss:  0.2064700871706009  accuracy:  0.9378999471664429\n",
      "loss:  0.18390204012393951  accuracy:  0.9455999732017517\n",
      "loss:  0.18480242788791656  accuracy:  0.9442999958992004\n",
      "loss:  0.1646227389574051  accuracy:  0.9505999684333801\n",
      "loss:  0.1499641239643097  accuracy:  0.9553999900817871\n",
      "loss:  0.1388562172651291  accuracy:  0.9596999883651733\n",
      "loss:  0.1377822458744049  accuracy:  0.9587000012397766\n",
      "loss:  0.12243318557739258  accuracy:  0.9632999897003174\n",
      "loss:  0.1265191286802292  accuracy:  0.9620999693870544\n",
      "loss:  0.13990972936153412  accuracy:  0.9577999711036682\n",
      "loss:  0.11808068305253983  accuracy:  0.9645999670028687\n",
      "loss:  0.11816150695085526  accuracy:  0.965399980545044\n",
      "loss:  0.1541401594877243  accuracy:  0.9514999985694885\n",
      "loss:  0.13229358196258545  accuracy:  0.9591999650001526\n",
      "loss:  0.10739582031965256  accuracy:  0.9680999517440796\n",
      "loss:  0.11017287522554398  accuracy:  0.9656999707221985\n",
      "loss:  0.09920153766870499  accuracy:  0.9702000021934509\n",
      "loss:  0.09534037113189697  accuracy:  0.97079998254776\n",
      "loss:  0.09724605828523636  accuracy:  0.9691999554634094\n",
      "loss:  0.11151079833507538  accuracy:  0.9667999744415283\n",
      "loss:  0.09318537265062332  accuracy:  0.9711999893188477\n",
      "loss:  0.1006021723151207  accuracy:  0.9693999886512756\n",
      "loss:  0.0884876698255539  accuracy:  0.9736999869346619\n",
      "loss:  0.08858433365821838  accuracy:  0.9727999567985535\n",
      "loss:  0.08877348899841309  accuracy:  0.9739999771118164\n",
      "loss:  0.08878478407859802  accuracy:  0.9736999869346619\n",
      "loss:  0.09193874150514603  accuracy:  0.973099946975708\n",
      "loss:  0.08756347745656967  accuracy:  0.9735999703407288\n",
      "loss:  0.08689602464437485  accuracy:  0.9736999869346619\n",
      "loss:  0.08203353732824326  accuracy:  0.9763000011444092\n",
      "loss:  0.08861324936151505  accuracy:  0.9734999537467957\n",
      "loss:  0.09488589316606522  accuracy:  0.9726999998092651\n",
      "loss:  0.08817652612924576  accuracy:  0.9746999740600586\n",
      "loss:  0.0846741795539856  accuracy:  0.9764999747276306\n",
      "loss:  0.08458217978477478  accuracy:  0.9747999906539917\n",
      "loss:  0.08767279982566833  accuracy:  0.9752999544143677\n",
      "loss:  0.09985648095607758  accuracy:  0.9706999659538269\n",
      "loss:  0.08580611646175385  accuracy:  0.974399983882904\n",
      "loss:  0.0891767293214798  accuracy:  0.9752999544143677\n",
      "loss:  0.08448471128940582  accuracy:  0.974299967288971\n",
      "loss:  0.08504757285118103  accuracy:  0.9756999611854553\n",
      "loss:  0.10567577928304672  accuracy:  0.9698999524116516\n",
      "loss:  0.08667467534542084  accuracy:  0.9763999581336975\n",
      "loss:  0.084681436419487  accuracy:  0.976099967956543\n",
      "loss:  0.09403853118419647  accuracy:  0.9732999801635742\n",
      "loss:  0.08756998181343079  accuracy:  0.975600004196167\n",
      "loss:  0.08335088193416595  accuracy:  0.9770999550819397\n",
      "loss:  0.12028995156288147  accuracy:  0.9682999849319458\n",
      "loss:  0.08633575588464737  accuracy:  0.976699948310852\n",
      "loss:  0.09350364655256271  accuracy:  0.975600004196167\n",
      "loss:  0.09359177947044373  accuracy:  0.9750999808311462\n",
      "loss:  0.08779335021972656  accuracy:  0.9764999747276306\n",
      "loss:  0.0931757465004921  accuracy:  0.976099967956543\n",
      "loss:  0.08632128685712814  accuracy:  0.9776999950408936\n"
     ]
    }
   ],
   "source": [
    "train_model(pytorch_net, criterion, optimizer, dataloaders, nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5bd0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "#torch.save(pytorch_net, 'models/my_digit_clasifier_3L_97pct.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e379c120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
