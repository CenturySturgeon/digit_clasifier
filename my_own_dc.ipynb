{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb1712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "! [ -e /content ] && pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4b759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c9df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d40a1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "094b7b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('../../.fastai/data/mnist_png/testing/5'),\n",
       " Path('../../.fastai/data/mnist_png/testing/3'),\n",
       " Path('../../.fastai/data/mnist_png/testing/7'),\n",
       " Path('../../.fastai/data/mnist_png/testing/4'),\n",
       " Path('../../.fastai/data/mnist_png/testing/2'),\n",
       " Path('../../.fastai/data/mnist_png/testing/1'),\n",
       " Path('../../.fastai/data/mnist_png/testing/9'),\n",
       " Path('../../.fastai/data/mnist_png/testing/0'),\n",
       " Path('../../.fastai/data/mnist_png/testing/8'),\n",
       " Path('../../.fastai/data/mnist_png/testing/6')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_folder = Path('../../.fastai/data/mnist_png/testing')\n",
    "training_folder = Path('../../.fastai/data/mnist_png/training')\n",
    "\n",
    "[x for x in testing_folder.iterdir() if x.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "331fd357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Path('../../.fastai/data/mnist_png/training/0'),\n",
       " Path('../../.fastai/data/mnist_png/testing/3'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_testing_folder = testing_folder.ls().sorted()\n",
    "sorted_training_folder = training_folder.ls().sorted()\n",
    "\n",
    "sorted_training_folder[0], sorted_testing_folder[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3537f2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6131, 28, 28]),\n",
       " torch.Size([6265, 28, 28]),\n",
       " torch.Size([12396, 784]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threes_tensors = [tensor(Image.open(o)) for o in (training_folder/'3').ls().sorted()]\n",
    "stacked_threes = torch.stack(threes_tensors).float()/255\n",
    "\n",
    "sevens_tensors = [tensor(Image.open(o)) for o in (training_folder/'7').ls().sorted()]\n",
    "stacked_sevens = torch.stack(sevens_tensors).float()/255\n",
    "\n",
    "#reshape the tensors and concatenate them in a new one\n",
    "test_tensor = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n",
    "\n",
    "stacked_threes.shape, stacked_sevens.shape, test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a4ff196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_tensors(path_to_images):\n",
    "    \"\"\"A function that returns a stacked Gray Scaled tensor\"\"\"\n",
    "    tensors = [tensor(Image.open(o)) for o in (path_to_images).ls().sorted()]\n",
    "    #Original line, only converts to grayscale \n",
    "    #stacked_tensors = torch.stack(tensors).float()/255\n",
    "    \n",
    "    # Transforms to grayscale (/255) and Normalizes to mean 0 ((x-0.5)/0.5)\n",
    "    stacked_tensors = ((torch.stack(tensors).float()/255)-.5)/.5\n",
    "    \n",
    "\n",
    "    return stacked_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9347be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the function works as expected\n",
    "training_stacked_threes = stacked_tensors(sorted_training_folder[3])\n",
    "training_stacked_sevens = stacked_tensors(sorted_training_folder[7])\n",
    "\n",
    "training_stacked_threes.shape, training_stacked_sevens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3866075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensors are not equal.\n"
     ]
    }
   ],
   "source": [
    "#Corroborate equality of stacked tensors\n",
    "if torch.equal(stacked_threes, training_stacked_threes) and torch.equal(stacked_sevens, training_stacked_sevens) :\n",
    "    print(\"The tensors are equal.\")\n",
    "else:\n",
    "    print(\"The tensors are not equal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00a60e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash_tensors(list_of_paths):\n",
    "    listed_tensors = []\n",
    "    for i in list_of_paths:\n",
    "        listed_tensors.append(stacked_tensors(i))\n",
    "    squashed_tensors = torch.cat(listed_tensors).view(-1, 28*28)\n",
    "    return squashed_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36fff3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash_tensors(list_of_paths):\n",
    "    squashed_tensors = torch.cat([stacked_tensors(o) for o in list_of_paths]).view(-1, 28*28)\n",
    "    return squashed_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a02dae31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the squashed tensors are equal\n",
    "squashed_threes_and_sevens = squash_tensors([sorted_training_folder[3], sorted_training_folder[7]])\n",
    "\n",
    "squashed_threes_and_sevens.shape == test_tensor.shape, torch.equal(squashed_threes_and_sevens, test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a08522a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 784])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squashed_training_tensors = squash_tensors(sorted_training_folder)\n",
    "\n",
    "squashed_training_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d2ad690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1010, 28, 28]),\n",
       " torch.Size([1028, 28, 28]),\n",
       " torch.Size([2038, 784]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid3s_tensor = torch.stack([tensor(Image.open(o)) for o in (sorted_testing_folder[3]).ls().sorted()]).float()/255\n",
    "valid7s_tensor = torch.stack([tensor(Image.open(o)) for o in (sorted_testing_folder[7]).ls().sorted()]).float()/255\n",
    "valid37s_tensor = torch.cat([valid3s_tensor, valid7s_tensor]).view(-1, 28*28)\n",
    "\n",
    "valid3s_tensor.shape, valid7s_tensor.shape, valid37s_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e639bd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1010, 28, 28]),\n",
       " torch.Size([1028, 28, 28]),\n",
       " torch.Size([2038, 784]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_stacked_threes = stacked_tensors(sorted_testing_folder[3])\n",
    "validation_stacked_sevens = stacked_tensors(sorted_testing_folder[7])\n",
    "squashed_valid3and7s_tensor = squash_tensors([sorted_testing_folder[3], sorted_testing_folder[7]])\n",
    "\n",
    "validation_stacked_threes.shape, validation_stacked_sevens.shape, squashed_valid3and7s_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b060be6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensors are not equal.\n"
     ]
    }
   ],
   "source": [
    "#Corroborate equality of stacked tensors\n",
    "if torch.equal(valid3s_tensor, validation_stacked_threes) and torch.equal(valid7s_tensor, validation_stacked_sevens) :\n",
    "    print(\"The tensors are equal.\")\n",
    "else:\n",
    "    print(\"The tensors are not equal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73319112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the squashed tensors are equal\n",
    "squashed_valid3and7s_tensor.shape == valid37s_tensor.shape, torch.equal(squashed_valid3and7s_tensor, valid37s_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e660b681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 784])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squashed_validation_tensors = squash_tensors(sorted_testing_folder)\n",
    "\n",
    "squashed_validation_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d96920b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test = tensor([1]*len(threes_tensors) + [0]*len(sevens_tensors)).unsqueeze(1)\n",
    "labels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bf63ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the validation labels tensor\n",
    "def getlabels(folders):\n",
    "    list_of_labels = []\n",
    "    for o in range(len(folders)):\n",
    "        list_of_labels = list_of_labels + [o]*len(folders[o].ls())\n",
    "    labels_tensor = tensor(list_of_labels)\n",
    "    return labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d693c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List comprehension version of the code block above\n",
    "#start_time = time.time()\n",
    "#a = [o for o in range(len(sorted_testing_folder)) for _ in range(len(sorted_testing_folder[o].ls()))]\n",
    "#tensor_a = torch.tensor(a)\n",
    "#for_loop_time = time.time() - start_time\n",
    "#for_loop_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "116ea797",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_labels_tensor = getlabels(sorted_testing_folder)\n",
    "\n",
    "training_labels_tensor = getlabels(sorted_training_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4861b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = list(zip(squashed_validation_tensors, validation_labels_tensor))\n",
    "\n",
    "training_dataset = list(zip(squashed_training_tensors, training_labels_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57cb3ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataloader = DataLoader(validation_dataset, batch_size = 256, shuffle=True)\n",
    "\n",
    "training_dataloader = DataLoader(training_dataset, batch_size = 256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38cc0d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": training_dataloader,\n",
    "    \"validation\": validation_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c910108",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_net = nn.Sequential(\n",
    "    #nn.Flatten changes the shape of the images, if not they would case the error:\n",
    "    #mat1 and mat2 shapes cannot be multiplied (28x28 and 784x128)\n",
    "\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,10),\n",
    "    nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7af816e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nb_epoch is our number of epochs, meaning the number of complete passes through the training dataset.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "lr = 1e-2\n",
    "nb_epoch = 65\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a8ac2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(pytorch_net.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2c92698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, num_epochs=10):\n",
    "    #liveloss = PlotLosses() Live training plot generic API\n",
    "    epochno = 0\n",
    "    model = model.to(device) # Moves and/or casts the parameters and buffers to device.\n",
    "    \n",
    "    for epoch in range(num_epochs): # Number of passes through the entire training & validation datasets\n",
    "        logs = {}\n",
    "        for phase in ['train', 'validation']: # First train, then validate\n",
    "            if phase == 'train':\n",
    "                model.train() # Set the module in training mode\n",
    "            else:\n",
    "                model.eval() # Set the module in evaluation mode\n",
    "\n",
    "            running_loss = 0.0 # keep track of loss\n",
    "            running_corrects = 0 # count of carrectly classified inputs\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device) # Perform Tensor device conversion\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs) # forward pass through network\n",
    "                loss = criterion(outputs, labels) # Calculate loss\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad() # Set all previously calculated gradients to 0\n",
    "                    loss.backward() # Calculate gradients\n",
    "                    optimizer.step() # Step on the weights using those gradient w -=  gradient(w) * lr\n",
    "\n",
    "                _, preds = torch.max(outputs, 1) # Get model's predictions\n",
    "                running_loss += loss.detach() * inputs.size(0) # multiply mean loss by the number of elements\n",
    "                running_corrects += torch.sum(preds == labels.data) # add number of correct predictions to total\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset) # get the \"mean\" loss for the epoch\n",
    "            epoch_acc = running_corrects.float() / len(dataloaders[phase].dataset) # Get proportion of correct predictions\n",
    "            \n",
    "            # Logging\n",
    "            prefix = ''\n",
    "            if phase == 'validation':\n",
    "                prefix = 'val_'\n",
    "\n",
    "            logs[prefix + 'log loss'] = epoch_loss.item()\n",
    "            logs[prefix + 'accuracy'] = epoch_acc.item()\n",
    "        print('Epoch: ', epochno,' loss: ', epoch_loss.item(), ' accuracy: ', epoch_acc.item())\n",
    "        epochno += 1\n",
    "        #liveloss.update(logs) Update logs\n",
    "        #liveloss.send()  draw, display stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1f2e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  loss:  0.14694856107234955  accuracy:  0.9577999711036682\n",
      "Epoch:  1  loss:  0.1454765647649765  accuracy:  0.9580000042915344\n",
      "Epoch:  2  loss:  0.1426377296447754  accuracy:  0.9575999975204468\n",
      "Epoch:  3  loss:  0.13559454679489136  accuracy:  0.9608999490737915\n",
      "Epoch:  4  loss:  0.1275307536125183  accuracy:  0.9626999497413635\n",
      "Epoch:  5  loss:  0.1370524764060974  accuracy:  0.9608999490737915\n",
      "Epoch:  6  loss:  0.12496857345104218  accuracy:  0.9642999768257141\n",
      "Epoch:  7  loss:  0.12100208550691605  accuracy:  0.9660999774932861\n",
      "Epoch:  8  loss:  0.12074313312768936  accuracy:  0.9648999571800232\n",
      "Epoch:  9  loss:  0.13153505325317383  accuracy:  0.964199960231781\n",
      "Epoch:  10  loss:  0.1191115453839302  accuracy:  0.9677000045776367\n",
      "Epoch:  11  loss:  0.11952011287212372  accuracy:  0.9660999774932861\n",
      "Epoch:  12  loss:  0.12076672166585922  accuracy:  0.9641000032424927\n",
      "Epoch:  13  loss:  0.13491882383823395  accuracy:  0.9596999883651733\n",
      "Epoch:  14  loss:  0.11361610144376755  accuracy:  0.9668999910354614\n",
      "Epoch:  15  loss:  0.1163332462310791  accuracy:  0.9650999903678894\n",
      "Epoch:  16  loss:  0.11334527283906937  accuracy:  0.9677000045776367\n",
      "Epoch:  17  loss:  0.11055603623390198  accuracy:  0.9684000015258789\n",
      "Epoch:  18  loss:  0.11630955338478088  accuracy:  0.9681999683380127\n",
      "Epoch:  19  loss:  0.11372190713882446  accuracy:  0.9662999510765076\n",
      "Epoch:  20  loss:  0.10962090641260147  accuracy:  0.9704999923706055\n",
      "Epoch:  21  loss:  0.10606315732002258  accuracy:  0.9695000052452087\n",
      "Epoch:  22  loss:  0.10441531985998154  accuracy:  0.9698999524116516\n",
      "Epoch:  23  loss:  0.10511206835508347  accuracy:  0.9706999659538269\n",
      "Epoch:  24  loss:  0.10461550205945969  accuracy:  0.9695999622344971\n",
      "Epoch:  25  loss:  0.10780978947877884  accuracy:  0.9692999720573425\n",
      "Epoch:  26  loss:  0.10658593475818634  accuracy:  0.9695999622344971\n",
      "Epoch:  27  loss:  0.10739017277956009  accuracy:  0.9680999517440796\n",
      "Epoch:  28  loss:  0.10016463696956635  accuracy:  0.972000002861023\n",
      "Epoch:  29  loss:  0.10133542120456696  accuracy:  0.971299946308136\n",
      "Epoch:  30  loss:  0.10443691164255142  accuracy:  0.9705999493598938\n",
      "Epoch:  31  loss:  0.10126332193613052  accuracy:  0.9706999659538269\n",
      "Epoch:  32  loss:  0.09958432614803314  accuracy:  0.9722999930381775\n",
      "Epoch:  33  loss:  0.10157141834497452  accuracy:  0.9713999629020691\n",
      "Epoch:  34  loss:  0.0988597497344017  accuracy:  0.9718999862670898\n",
      "Epoch:  35  loss:  0.10225880146026611  accuracy:  0.9706999659538269\n",
      "Epoch:  36  loss:  0.09969894587993622  accuracy:  0.9716999530792236\n",
      "Epoch:  37  loss:  0.10624626278877258  accuracy:  0.968999981880188\n",
      "Epoch:  38  loss:  0.10233081132173538  accuracy:  0.9696999788284302\n",
      "Epoch:  39  loss:  0.100751593708992  accuracy:  0.9728999733924866\n",
      "Epoch:  40  loss:  0.0961606353521347  accuracy:  0.9735999703407288\n",
      "Epoch:  41  loss:  0.09893250465393066  accuracy:  0.9722999930381775\n",
      "Epoch:  42  loss:  0.0941648855805397  accuracy:  0.9745999574661255\n",
      "Epoch:  43  loss:  0.09442687779664993  accuracy:  0.9735999703407288\n",
      "Epoch:  44  loss:  0.0922279879450798  accuracy:  0.9741999506950378\n",
      "Epoch:  45  loss:  0.09731997549533844  accuracy:  0.9732999801635742\n",
      "Epoch:  46  loss:  0.0968429446220398  accuracy:  0.9713999629020691\n",
      "Epoch:  47  loss:  0.11462338268756866  accuracy:  0.9672999978065491\n",
      "Epoch:  48  loss:  0.09205568581819534  accuracy:  0.97489994764328\n",
      "Epoch:  49  loss:  0.0973786860704422  accuracy:  0.971299946308136\n",
      "Epoch:  50  loss:  0.11166584491729736  accuracy:  0.9664999842643738\n",
      "Epoch:  51  loss:  0.09140858799219131  accuracy:  0.9749999642372131\n",
      "Epoch:  52  loss:  0.09159133583307266  accuracy:  0.9758999943733215\n",
      "Epoch:  53  loss:  0.09225743263959885  accuracy:  0.9739999771118164\n",
      "Epoch:  54  loss:  0.09124792367219925  accuracy:  0.9736999869346619\n",
      "Epoch:  55  loss:  0.09407477080821991  accuracy:  0.9734999537467957\n",
      "Epoch:  56  loss:  0.09438702464103699  accuracy:  0.9732999801635742\n",
      "Epoch:  57  loss:  0.09187458455562592  accuracy:  0.9734999537467957\n",
      "Epoch:  58  loss:  0.09019250422716141  accuracy:  0.97489994764328\n",
      "Epoch:  59  loss:  0.09773601591587067  accuracy:  0.9718999862670898\n",
      "Epoch:  60  loss:  0.09269183874130249  accuracy:  0.9734999537467957\n",
      "Epoch:  61  loss:  0.09009246528148651  accuracy:  0.9749999642372131\n",
      "Epoch:  62  loss:  0.08893588930368423  accuracy:  0.9752999544143677\n",
      "Epoch:  63  loss:  0.08895924687385559  accuracy:  0.9754999876022339\n",
      "Epoch:  64  loss:  0.08906950801610947  accuracy:  0.9754999876022339\n"
     ]
    }
   ],
   "source": [
    "train_model(pytorch_net, criterion, optimizer, dataloaders, nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "122744f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pytorch_net, 'models/my_own_dc_97pct.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22139857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ddee7559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Started developing the lower level for the rest of the model\n",
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11cb7b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_params((28*28, 10))\n",
    "bias = init_params(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1bcee681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-144.3747, -142.3893, -143.0893, -144.4469, -145.3126, -140.7037, -141.4348, -144.3125, -142.0406, -142.9065], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_prediction = (squashed_training_tensors[0]*weights.T).sum() + bias\n",
    "my_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "facdc195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 784])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear1(batch): return batch@weights + bias\n",
    "squashed_training_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a1978a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = linear1(squashed_training_tensors)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6fc39f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(35.4314, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.NLLLoss()\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "loss(m(predictions), training_labels_tensor.long())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f8d1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c113f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3581019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.Grayscale(), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize([0.5], [0.5])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4050dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = torchvision.datasets.ImageFolder((training_folder).as_posix(), transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d68ce54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f9021d5ffd0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchSize = 64\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=batchSize, shuffle=True)\n",
    "\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "641affe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f9021d5ef50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing_dataset = torchvision.datasets.ImageFolder(testing_folder, transform = transform)\n",
    "#testing_dataloader = torch.utils.data.DataLoader(testing_dataset, batch_size=batchSize)\n",
    "testing_dataset = torchvision.datasets.ImageFolder((testing_folder).as_posix(), transform = transform)\n",
    "testing_dataloader = torch.utils.data.DataLoader(testing_dataset, batch_size=batchSize)\n",
    "\n",
    "testing_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db009b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"validation\": testing_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af43c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,10),\n",
    "    nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f58720c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nb_epoch is our number of epochs, meaning the number of complete passes through the training dataset.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "lr = 1e-2\n",
    "nb_epoch = 60\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07b9138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(pytorch_net.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef2c4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, num_epochs=10):\n",
    "    #liveloss = PlotLosses() Live training plot generic API\n",
    "    model = model.to(device) # Moves and/or casts the parameters and buffers to device.\n",
    "    \n",
    "    for epoch in range(num_epochs): # Number of passes through the entire training & validation datasets\n",
    "        logs = {}\n",
    "        for phase in ['train', 'validation']: # First train, then validate\n",
    "            if phase == 'train':\n",
    "                model.train() # Set the module in training mode\n",
    "            else:\n",
    "                model.eval() # Set the module in evaluation mode\n",
    "\n",
    "            running_loss = 0.0 # keep track of loss\n",
    "            running_corrects = 0 # count of carrectly classified inputs\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device) # Perform Tensor device conversion\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs) # forward pass through network\n",
    "                loss = criterion(outputs, labels) # Calculate loss\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad() # Set all previously calculated gradients to 0\n",
    "                    loss.backward() # Calculate gradients\n",
    "                    optimizer.step() # Step on the weights using those gradient w -=  gradient(w) * lr\n",
    "\n",
    "                _, preds = torch.max(outputs, 1) # Get model's predictions\n",
    "                running_loss += loss.detach() * inputs.size(0) # multiply mean loss by the number of elements\n",
    "                running_corrects += torch.sum(preds == labels.data) # add number of correct predictions to total\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset) # get the \"mean\" loss for the epoch\n",
    "            epoch_acc = running_corrects.float() / len(dataloaders[phase].dataset) # Get proportion of correct predictions\n",
    "            \n",
    "            # Logging\n",
    "            prefix = ''\n",
    "            if phase == 'validation':\n",
    "                prefix = 'val_'\n",
    "\n",
    "            logs[prefix + 'log loss'] = epoch_loss.item()\n",
    "            logs[prefix + 'accuracy'] = epoch_acc.item()\n",
    "        print('loss: ', epoch_loss.item(), ' accuracy: ', epoch_acc.item())\n",
    "        \n",
    "        #liveloss.update(logs) Update logs\n",
    "        #liveloss.send()  draw, display stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f790dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.593288242816925  accuracy:  0.8185999989509583\n",
      "loss:  0.4740932881832123  accuracy:  0.8499999642372131\n",
      "loss:  0.32599714398384094  accuracy:  0.9025999903678894\n",
      "loss:  0.30259695649147034  accuracy:  0.9103999733924866\n",
      "loss:  0.25638362765312195  accuracy:  0.923799991607666\n",
      "loss:  0.22863425314426422  accuracy:  0.9310999512672424\n",
      "loss:  0.2064700871706009  accuracy:  0.9378999471664429\n",
      "loss:  0.18390204012393951  accuracy:  0.9455999732017517\n",
      "loss:  0.18480242788791656  accuracy:  0.9442999958992004\n",
      "loss:  0.1646227389574051  accuracy:  0.9505999684333801\n",
      "loss:  0.1499641239643097  accuracy:  0.9553999900817871\n",
      "loss:  0.1388562172651291  accuracy:  0.9596999883651733\n",
      "loss:  0.1377822458744049  accuracy:  0.9587000012397766\n",
      "loss:  0.12243318557739258  accuracy:  0.9632999897003174\n",
      "loss:  0.1265191286802292  accuracy:  0.9620999693870544\n",
      "loss:  0.13990972936153412  accuracy:  0.9577999711036682\n",
      "loss:  0.11808068305253983  accuracy:  0.9645999670028687\n",
      "loss:  0.11816150695085526  accuracy:  0.965399980545044\n",
      "loss:  0.1541401594877243  accuracy:  0.9514999985694885\n",
      "loss:  0.13229358196258545  accuracy:  0.9591999650001526\n",
      "loss:  0.10739582031965256  accuracy:  0.9680999517440796\n",
      "loss:  0.11017287522554398  accuracy:  0.9656999707221985\n",
      "loss:  0.09920153766870499  accuracy:  0.9702000021934509\n",
      "loss:  0.09534037113189697  accuracy:  0.97079998254776\n",
      "loss:  0.09724605828523636  accuracy:  0.9691999554634094\n",
      "loss:  0.11151079833507538  accuracy:  0.9667999744415283\n",
      "loss:  0.09318537265062332  accuracy:  0.9711999893188477\n",
      "loss:  0.1006021723151207  accuracy:  0.9693999886512756\n",
      "loss:  0.0884876698255539  accuracy:  0.9736999869346619\n",
      "loss:  0.08858433365821838  accuracy:  0.9727999567985535\n",
      "loss:  0.08877348899841309  accuracy:  0.9739999771118164\n",
      "loss:  0.08878478407859802  accuracy:  0.9736999869346619\n",
      "loss:  0.09193874150514603  accuracy:  0.973099946975708\n",
      "loss:  0.08756347745656967  accuracy:  0.9735999703407288\n",
      "loss:  0.08689602464437485  accuracy:  0.9736999869346619\n",
      "loss:  0.08203353732824326  accuracy:  0.9763000011444092\n",
      "loss:  0.08861324936151505  accuracy:  0.9734999537467957\n",
      "loss:  0.09488589316606522  accuracy:  0.9726999998092651\n",
      "loss:  0.08817652612924576  accuracy:  0.9746999740600586\n",
      "loss:  0.0846741795539856  accuracy:  0.9764999747276306\n",
      "loss:  0.08458217978477478  accuracy:  0.9747999906539917\n",
      "loss:  0.08767279982566833  accuracy:  0.9752999544143677\n",
      "loss:  0.09985648095607758  accuracy:  0.9706999659538269\n",
      "loss:  0.08580611646175385  accuracy:  0.974399983882904\n",
      "loss:  0.0891767293214798  accuracy:  0.9752999544143677\n",
      "loss:  0.08448471128940582  accuracy:  0.974299967288971\n",
      "loss:  0.08504757285118103  accuracy:  0.9756999611854553\n",
      "loss:  0.10567577928304672  accuracy:  0.9698999524116516\n",
      "loss:  0.08667467534542084  accuracy:  0.9763999581336975\n",
      "loss:  0.084681436419487  accuracy:  0.976099967956543\n",
      "loss:  0.09403853118419647  accuracy:  0.9732999801635742\n",
      "loss:  0.08756998181343079  accuracy:  0.975600004196167\n",
      "loss:  0.08335088193416595  accuracy:  0.9770999550819397\n",
      "loss:  0.12028995156288147  accuracy:  0.9682999849319458\n",
      "loss:  0.08633575588464737  accuracy:  0.976699948310852\n",
      "loss:  0.09350364655256271  accuracy:  0.975600004196167\n",
      "loss:  0.09359177947044373  accuracy:  0.9750999808311462\n",
      "loss:  0.08779335021972656  accuracy:  0.9764999747276306\n",
      "loss:  0.0931757465004921  accuracy:  0.976099967956543\n",
      "loss:  0.08632128685712814  accuracy:  0.9776999950408936\n"
     ]
    }
   ],
   "source": [
    "train_model(pytorch_net, criterion, optimizer, dataloaders, nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5bd0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "#torch.save(pytorch_net, 'models/my_digit_clasifier_3L_97pct.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e379c120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
