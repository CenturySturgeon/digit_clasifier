{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb1712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "! [ -e /content ] && pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4b759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c9df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d40a1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "094b7b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('../../.fastai/data/mnist_png/testing/5'),\n",
       " Path('../../.fastai/data/mnist_png/testing/3'),\n",
       " Path('../../.fastai/data/mnist_png/testing/7'),\n",
       " Path('../../.fastai/data/mnist_png/testing/4'),\n",
       " Path('../../.fastai/data/mnist_png/testing/2'),\n",
       " Path('../../.fastai/data/mnist_png/testing/1'),\n",
       " Path('../../.fastai/data/mnist_png/testing/9'),\n",
       " Path('../../.fastai/data/mnist_png/testing/0'),\n",
       " Path('../../.fastai/data/mnist_png/testing/8'),\n",
       " Path('../../.fastai/data/mnist_png/testing/6')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_folder = Path('../../.fastai/data/mnist_png/testing')\n",
    "training_folder = Path('../../.fastai/data/mnist_png/training')\n",
    "\n",
    "[x for x in testing_folder.iterdir() if x.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "331fd357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Path('../../.fastai/data/mnist_png/training/0'),\n",
       " Path('../../.fastai/data/mnist_png/testing/3'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_testing_folder = testing_folder.ls().sorted()\n",
    "sorted_training_folder = training_folder.ls().sorted()\n",
    "\n",
    "sorted_training_folder[0], sorted_testing_folder[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3537f2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6131, 28, 28]),\n",
       " torch.Size([6265, 28, 28]),\n",
       " torch.Size([12396, 784]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threes_tensors = [tensor(Image.open(o)) for o in (training_folder/'3').ls().sorted()]\n",
    "stacked_threes = torch.stack(threes_tensors).float()/255\n",
    "\n",
    "sevens_tensors = [tensor(Image.open(o)) for o in (training_folder/'7').ls().sorted()]\n",
    "stacked_sevens = torch.stack(sevens_tensors).float()/255\n",
    "\n",
    "#reshape the tensors and concatenate them in a new one\n",
    "test_tensor = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n",
    "\n",
    "stacked_threes.shape, stacked_sevens.shape, test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a4ff196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_tensors(path_to_images):\n",
    "    \"\"\"A function that returns a stacked Gray Scaled tensor\"\"\"\n",
    "    tensors = [tensor(Image.open(o)) for o in (path_to_images).ls().sorted()]\n",
    "    stacked_tensors = torch.stack(tensors).float()/255\n",
    "    return stacked_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9347be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the function works as expected\n",
    "training_stacked_threes = stacked_tensors(sorted_training_folder[3])\n",
    "training_stacked_sevens = stacked_tensors(sorted_training_folder[7])\n",
    "\n",
    "training_stacked_threes.shape, training_stacked_sevens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3866075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensors are equal.\n"
     ]
    }
   ],
   "source": [
    "#Corroborate equality of stacked tensors\n",
    "if torch.equal(stacked_threes, training_stacked_threes) and torch.equal(stacked_sevens, training_stacked_sevens) :\n",
    "    print(\"The tensors are equal.\")\n",
    "else:\n",
    "    print(\"The tensors are not equal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00a60e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash_tensors(list_of_paths):\n",
    "    listed_tensors = []\n",
    "    for i in list_of_paths:\n",
    "        listed_tensors.append(stacked_tensors(i))\n",
    "    squashed_tensors = torch.cat(listed_tensors).view(-1, 28*28)\n",
    "    return squashed_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a02dae31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the squashed tensors are equal\n",
    "squashed_threes_and_sevens = squash_tensors([sorted_training_folder[3], sorted_training_folder[7]])\n",
    "\n",
    "squashed_threes_and_sevens.shape == test_tensor.shape, torch.equal(squashed_threes_and_sevens, test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a08522a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 784])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squashed_training_tensors = squash_tensors(sorted_training_folder)\n",
    "\n",
    "squashed_training_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d2ad690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1010, 28, 28]),\n",
       " torch.Size([1028, 28, 28]),\n",
       " torch.Size([2038, 784]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid3s_tensor = torch.stack([tensor(Image.open(o)) for o in (sorted_testing_folder[3]).ls().sorted()]).float()/255\n",
    "valid7s_tensor = torch.stack([tensor(Image.open(o)) for o in (sorted_testing_folder[7]).ls().sorted()]).float()/255\n",
    "valid37s_tensor = torch.cat([valid3s_tensor, valid7s_tensor]).view(-1, 28*28)\n",
    "\n",
    "valid3s_tensor.shape, valid7s_tensor.shape, valid37s_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e639bd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1010, 28, 28]),\n",
       " torch.Size([1028, 28, 28]),\n",
       " torch.Size([2038, 784]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_stacked_threes = stacked_tensors(sorted_testing_folder[3])\n",
    "validation_stacked_sevens = stacked_tensors(sorted_testing_folder[7])\n",
    "squashed_valid3and7s_tensor = squash_tensors([sorted_testing_folder[3], sorted_testing_folder[7]])\n",
    "\n",
    "validation_stacked_threes.shape, validation_stacked_sevens.shape, squashed_valid3and7s_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b060be6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensors are equal.\n"
     ]
    }
   ],
   "source": [
    "#Corroborate equality of stacked tensors\n",
    "if torch.equal(valid3s_tensor, validation_stacked_threes) and torch.equal(valid7s_tensor, validation_stacked_sevens) :\n",
    "    print(\"The tensors are equal.\")\n",
    "else:\n",
    "    print(\"The tensors are not equal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73319112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the squashed tensors are equal\n",
    "squashed_valid3and7s_tensor.shape == valid37s_tensor.shape, torch.equal(squashed_valid3and7s_tensor, valid37s_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf15b9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 784])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squashed_validation_tensors = squash_tensors(sorted_testing_folder)\n",
    "\n",
    "squashed_validation_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a76260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3581019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.Grayscale(), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize([0.5], [0.5])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4050dba3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImageFolder' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m training_dataset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mImageFolder((training_folder)\u001b[38;5;241m.\u001b[39mas_posix(), transform \u001b[38;5;241m=\u001b[39m transform)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtraining_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageFolder' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "training_dataset = torchvision.datasets.ImageFolder((training_folder).as_posix(), transform = transform)\n",
    "training_dataset.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d68ce54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f02d8d33bb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchSize = 64\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=batchSize, shuffle=True)\n",
    "\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "641affe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f02d8d33e20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_dataset = torchvision.datasets.ImageFolder(testing_folder, transform = transform)\n",
    "testing_dataloader = torch.utils.data.DataLoader(testing_dataset, batch_size=batchSize)\n",
    "\n",
    "testing_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db009b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"validation\": testing_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af43c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_net = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,10),\n",
    "    nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f58720c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nb_epoch is our number of epochs, meaning the number of complete passes through the training dataset.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "lr = 1e-2\n",
    "nb_epoch = 15\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07b9138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(pytorch_net.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef2c4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, num_epochs=10):\n",
    "    #liveloss = PlotLosses() Live training plot generic API\n",
    "    model = model.to(device) # Moves and/or casts the parameters and buffers to device.\n",
    "    \n",
    "    for epoch in range(num_epochs): # Number of passes through the entire training & validation datasets\n",
    "        logs = {}\n",
    "        for phase in ['train', 'validation']: # First train, then validate\n",
    "            if phase == 'train':\n",
    "                model.train() # Set the module in training mode\n",
    "            else:\n",
    "                model.eval() # Set the module in evaluation mode\n",
    "\n",
    "            running_loss = 0.0 # keep track of loss\n",
    "            running_corrects = 0 # count of carrectly classified inputs\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device) # Perform Tensor device conversion\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs) # forward pass through network\n",
    "                loss = criterion(outputs, labels) # Calculate loss\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad() # Set all previously calculated gradients to 0\n",
    "                    loss.backward() # Calculate gradients\n",
    "                    optimizer.step() # Step on the weights using those gradient w -=  gradient(w) * lr\n",
    "\n",
    "                _, preds = torch.max(outputs, 1) # Get model's predictions\n",
    "                running_loss += loss.detach() * inputs.size(0) # multiply mean loss by the number of elements\n",
    "                running_corrects += torch.sum(preds == labels.data) # add number of correct predictions to total\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset) # get the \"mean\" loss for the epoch\n",
    "            epoch_acc = running_corrects.float() / len(dataloaders[phase].dataset) # Get proportion of correct predictions\n",
    "            \n",
    "            # Logging\n",
    "            prefix = ''\n",
    "            if phase == 'validation':\n",
    "                prefix = 'val_'\n",
    "\n",
    "            logs[prefix + 'log loss'] = epoch_loss.item()\n",
    "            logs[prefix + 'accuracy'] = epoch_acc.item()\n",
    "        print('loss: ', epoch_loss.item(), ' accuracy: ', epoch_acc.item())\n",
    "        \n",
    "        #liveloss.update(logs) Update logs\n",
    "        #liveloss.send()  draw, display stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f790dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.4052080810070038  accuracy:  0.8804000020027161\n",
      "loss:  0.34317728877067566  accuracy:  0.8989999890327454\n",
      "loss:  0.3060593008995056  accuracy:  0.9096999764442444\n",
      "loss:  0.2595570385456085  accuracy:  0.9241999983787537\n",
      "loss:  0.23294584453105927  accuracy:  0.9304999709129333\n",
      "loss:  0.21377748250961304  accuracy:  0.9378999471664429\n",
      "loss:  0.1968572586774826  accuracy:  0.9429000020027161\n",
      "loss:  0.17762190103530884  accuracy:  0.9496999979019165\n",
      "loss:  0.16019994020462036  accuracy:  0.9524999856948853\n",
      "loss:  0.1489601582288742  accuracy:  0.9549999833106995\n",
      "loss:  0.1422155797481537  accuracy:  0.9583999514579773\n",
      "loss:  0.14073260128498077  accuracy:  0.957099974155426\n",
      "loss:  0.15762324631214142  accuracy:  0.949999988079071\n",
      "loss:  0.13402201235294342  accuracy:  0.9598000049591064\n",
      "loss:  0.12043929100036621  accuracy:  0.9657999873161316\n"
     ]
    }
   ],
   "source": [
    "train_model(pytorch_net, criterion, optimizer, dataloaders, nb_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5bd0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "#torch.save(pytorch_net, 'models/my_digit_clasifier_3L_97pct.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e379c120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
